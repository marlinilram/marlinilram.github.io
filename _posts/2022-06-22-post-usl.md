---
title: '[PaperReading] Learning 3D Object Shape and Layout without 3D Supervision, CVPR 2022'
date: 2022-06-22
permalink: /posts/2022/06/usl/
excerpt: ""
tags:
  - 3d reconstruction
  - scene understanding
---

## Motivation:  
Understanding this 3D scene structure is critical for navigating or interacting with the world. Unfortunately, directly measuring or perceiving 3D structure is often impractical. For this reason, inferring the shape and layout of 3D scenes from 2D images has long been a fundamental problem in computer vision, with wide applications in robotics, autonomous vehicles, graphics, AR/VR, and beyond.

## Approach:  
build on Mesh R-CNN and replace expensive mesh supervision with scalable multi-view supervision. To also predict layout we use a *distance transform loss*.

大部分相关工作需要依赖3D标注信息作为监督信号来训练网络, 但是3D标注过程繁琐复杂, 因此本文使用多视角RGB图像的2D轮廓图作为训练监督. 

backbone(ResNet50-FPN)输出feature map
RPN(Fast R-CNN)输出RoI proposal
每个RoI送入
1. box head: 预测2D包围盒与类目
2. layout head: 预测物体在图像深度方向的长度以及中心点深度
3. shape head: 预测三角网格

box head与layout head通过RoIAlign对feature map进行crop和resize
shape head通过RoIMap, 此处相比Mesh R-CNN的改进是直接从feature map进行vertex的feature采样而不是先经过RoIAlign变换后再采样, 从而避免重复插值并且能根据实际投影形状来进行计算

通过多视角轮廓图进行训练
轮廓图的pixel-wise CE loss对于shape不正确能给出较好的梯度, 但是对于shape正确layout不正确时, 需要使用*distance transform loss*, 用于惩罚两个mask之间的2D距离
$$l_{dist}(\hat{S}, S)=\int_{\hat{S}} \underset{s \in S}{inf}\lVert \hat{s}-s \rVert^2d\hat{s} + \int_{S} \underset{\hat{s} \in \hat{S}}{inf}\lVert \hat{s}-s \rVert^2ds$$